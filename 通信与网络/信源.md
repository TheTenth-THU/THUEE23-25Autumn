## 信源的概念

> [!definition] 信源
> 信息的产生者称为**信源 (source of information)**，产生一个**随机过程**发出信息。

### 离散无记忆信源

**离散信源 (discrete source)** 是时间、取值上都离散的信源，可以表示为 
$$
X[k] \in \{ x_{1}, x_{2}, \cdots, x_{N} \}
$$

为简化讨论，可以假设 **$X[k]$ 是独立同分布的随机过程**，即：

> [!definition] 离散无记忆信源
> 持续产生**独立同分布**的符号  $X \in \{ x_{1}, x_{2}, \cdots, x_{N} \}$ 的信源，称为**离散无记忆信源 (discrete memoryless source, DMS)**，记为
> $$
> X \sim \begin{pmatrix}
> x_{1} & x_{2} & \cdots & x_{N} \\
> p_{1} & p_{2} & \cdots & p_{N}
> \end{pmatrix}, \quad
> p_{i} = \Pr\{ X = x_{i} \}
> $$

### 信源编码的基本要求

将信源产生的符号 $X$ 映射为 0, 1 比特串 $f(X)$ 的过程称为**信源编码 (source coding)**，产生的比特串称为**码字 (codeword)**，其长度 $l$ 称为**码长 (code length)**。

+ 若不同 $x_{i}$ 映射出的码长 $l_{i}$ 相同，称为**定长码 (fixed-length code)**；
+ 若不同 $x_{i}$ 映射出的码长 $l_{i}$ 不同，称为**变长码 (variable-length code)**。

#### 定长码的可解码条件

对**定长码**，要求
$$
f(x_{i}) \neq f(x_{j}), \quad \forall i \neq j
$$
因此固定码长 $l$ 应满足
$$
N \leq 2^{l} \implies l \geq \lceil \log N \rceil
$$
此处 $\log$ 均为以 2 为底的对数。

#### 变长码的可解码条件

对**变长码**，要求任意码字不能是另一个码字的前缀，因此又称为**前缀码 (prefix code)**。

引入**平均码长 (average code length)** 
$$
\bar{l} = \sum\limits_{i=1}^{N} p_{i} l_{i}
$$
我们希望在可解码的条件下，使 $\bar{l}$ 尽可能小。

可以证明，信源 $X \sim \begin{pmatrix} x_{1} & x_{2} & \cdots & x_{N} \\ p_{1} & p_{2} & \cdots & p_{N} \end{pmatrix}$ 的最小平均码长为
$$
\bar{l}_{\min} = - \sum\limits_{i=1}^{N} p_{i} \log p_{i}
$$

## 信源的熵

### 离散信源的熵

> [!definition] 离散信源的熵
> 离散无记忆信源 $X \sim \begin{pmatrix} x_{1} & x_{2} & \cdots & x_{N} \\ p_{1} & p_{2} & \cdots & p_{N} \end{pmatrix}$ 的**熵 (entropy)** 定义为
> $$
> H(X) = - \sum\limits_{i=1}^{N} p_{i} \log p_{i}
> $$

$H(X)$ 可以写为
$$
H(X) = \mathbb{E}_{X} [ - \log p(X) ]
$$
因而 $- \log \Pr\{ X = x_{i} \}$ 刻画事件 $\{ X = x_{i} \}$ 所包含的信息量，**概率越小，信息量越大**。

#### 熵的性质

考虑 $H(X) = \bar{l}_{\min}$ 的物理含义，应有 $0 \le H(X) \le \log N$。

+ 当且仅当 $$\exists i \in \{ 1, 2, \cdots,N \}, \quad p_{i} = 1$$ 时，$H(X) = 0$，此时信源没有不确定性，不包含信息；

+ 当且仅当 $$p_{i} = \dfrac{1}{N}, \quad i = 1, 2, \cdots, N$$ 时，$H(X) = \log N$，此时信源不确定性最大，包含信息量最大。
	对离散信源，**均匀分布的不确定度最大**。

#### 联合熵、条件熵

> [!definition] 联合熵
> 考虑两个离散无记忆信源 $X$ 和 $Y$，事件 $\{ X = x_{i}, Y = y_{j} \}$ 的概率为 $p_{ij} = \Pr\{ X = x_{i}, Y = y_{j} \}$，则 $X$ 和 $Y$ 的**联合熵 (joint entropy)** 定义为
> $$
> H(X, Y) = \mathbb{E}_{XY} [ - \log p(X, Y) ]
> = - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log p_{ij}
> $$
> 在没有歧义的情况下，联合熵 $H(X,Y)$ 可记为 $H(XY)$。

联合熵刻画了将 $X$ 和 $Y$ 一起编码所需的最小平均码长，即综合考虑 $X$ 和 $Y$ 的信息量。显然，$H(X,Y) \ge H(X)$。

> [!definition] 条件熵
> 考虑 $X$ 和 $Y$ 的联合分布 $p_{ij} = \Pr\{ X = x_{i}, Y = y_{j} \}$，在 $Y$ 已知的条件下，事件 $\{ X = x_{i} \}$ 发生的概率为
> $$
> p_{i \mid j} = \Pr\{ X = x_{i} \mid Y = y_{j} \} = \dfrac{\Pr\{ X = x_{i}, Y = y_{j} \}}{\Pr\{ Y = y_{j} \}} = \dfrac{p_{ij}}{p_{j}}
> $$
> 则以 $Y$ 为条件的 $X$ 的**条件熵 (conditional entropy)** 定义为
> $$
> H(X \mid Y) = \mathbb{E}_{XY} [ - \log p(X \mid Y) ] = - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log p_{i \mid j}
> $$

条件熵 $H(X \mid Y)$ 刻画了在已知 $Y$ 的条件下，$X$ 所包含的信息量，即在观测 $Y$ 后 $X$ 残存的不确定度。显然， $0 \le H(X \mid Y) \le H(X)$。

> [!note] 熵的通信意义
> 信源的熵 $H(X)$ 刻画了信源 $X$ 所包含的信息量，其值即对 $X$ 进行无失真编码时所需的最小平均码长。
> 
> + 考虑对一个离散无记忆信源 $X$ 编码传输，当**信道速率**（每传输一个信源符号所传输的平均比特数）**$R \geq H(X)$** 时，可以实现**无失真**传输，即信源译码环节可无失真恢复 $X$；
> 
> + 考虑对两个离散无记忆信源 $X$ 和 $Y$ 做联合信源编码传输，当**信道速率 $R \geq H(X, Y)$** 时，在信源译码环节可无失真恢复 $(X, Y)$；
> 
> + 考虑对离散无记忆信源 $X$ 编码传输，编译码器可以共同观测另一个信源 ，当**信道速率 $R \geq H(X \mid Y)$** 时，在信源译码环节可无失真恢复 。

#### 熵的链式法则

> [!theorem] 熵的链式法则
> 对于任意两个离散随机变量 $X$ 和 $Y$，有
> $$
> H(X, Y) = H(Y) + H(X \mid Y) = H(X) + H(Y \mid X)
> $$

进一步地，

+ 若 **$X$ 与 $Y$ 独立**（记为 $X \perp Y$），则 $p_{ij} = p_{i}p_{j}$，$p_{i \mid j} = \dfrac{p_{i}p_{j}}{p_{j}} = p_{i}$，于是
$$
\begin{align}
H(X \mid Y) &= - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log p_{i \mid j} = - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{i} p_{j} \log p_{i}  \\
&= - \sum\limits_{i=1}^{N} p_{i} \log p_{i} = H(X)
\end{align}
$$
$$
H(X, Y) = H(X \mid Y) + H(Y) = H(X) + H(Y)
$$
即**独立随机变量的联合熵等于各自熵之和**。观测 $Y$ 不会减少 $X$ 的不确定性。

+ 若 $X$ 是 $Y$ 的**确定性映射**（记为 $X = f(Y)$），则 $p_{i \mid j}$ 要么为 0，要么为 1，因此 $H(X \mid Y) = 0$，于是
$$
H(X, Y) = H(Y)
$$
即**确定性映射不会增加不确定性**。观测 $Y$ 会完全消除 $X$ 的不确定性。

进而，一般地有
+ $H(X+Y \mid Y) = H(X \mid Y)$；
+ $H((X+Y), X) = H(X) + H(X + Y \mid X) = H(X) + H(Y \mid X) = H(X, Y)$。 


#### 互信息

$H(X \mid Y)$ 表征通过观测 $Y$ 后 $X$ 残存的不确定度，这种观测所消除不确定度的程度，即称为**互信息 (mutual information)**。

显然，有
$$
\begin{align}
H(X) - H(X \mid Y) &= H(X) - \big( H(X, Y) - H(Y) \big) \\
&= H(X) + H(Y) - H(X, Y) \\
&= H(Y) - H(Y \mid X)
\end{align}
$$
即，通过观测 $Y$ 消除的 $X$ 的不确定度，等于通过观测 $X$ 消除的 $Y$ 的不确定度。$X$ 与 $Y$ 的**互信息是对称的**。

> [!definition] 互信息
> 离散随机变量 $X$ 和 $Y$ 的**互信息 (mutual information)** 定义为
> $$
> I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)
> $$

一般地，
$$
I(X; Y) = H(X) + H(Y) - H(X, Y)
= \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log \dfrac{p_{ij}}{p_{i} p_{j}}
$$

考虑两个离散无记忆信源 $X$ 和 $Y$，

+ 若 $X \perp Y$，则
$$
I(X; Y) = H(X) - H(X \mid Y) = H(X) - H(X) = 0
$$
即**独立信源间没有互信息**；另可证明，没有互信息的信源是独立的。

+ 若 $X = f(Y)$，则
$$
I(X; Y) = H(X) - H(X \mid Y) = H(X) - 0 = H(X)
$$
即**确定性映射的信源间互信息是其全部不确定度**。 

+ $0 \le H(X \mid Y) \le H(X)$，因此 $0 \le I(X;Y) \le \min \left\{ H(X), H(Y) \right\}$，**信源之间不存在欺骗**。

### 连续信源的微分熵

对于连续随机变量 $X$，其概率分布由**概率密度函数 (probability density function, PDF)** $p_{X}(x)$ 描述，满足
$$
\dint_{-\infty}^{+\infty} p_{X}(x) \dif x = 1, \quad p_{X}(x) \geq 0
$$

> [!definition] 微分熵
> **微分熵 (differential entropy)** 定义为
> $$
> h(X) = \mathbb{E}_{X} [ - \log p_{X}(X) ] = - \dint_{-\infty}^{+\infty} p_{X}(x) \log p_{X}(x)  \dif x
> $$

微分熵 $h(X)$ **刻画了信源 $X$ 的相对不确定度**，其值可以为负数。

> [!example]- 均匀分布的微分熵
> 对 $X \sim \mathcal{U}(a, b)$，有 $p_{X}(x) = \dfrac{1}{b - a}$，因此
> $$
> h(X) = - \dint_{a}^{b} p_{X}(x) \log p_{X}(x) \dif x = - \dint_{a}^{b} \dfrac{1}{b - a} \log \dfrac{1}{b - a} \dif x = \mark{ \log (b - a) }
> $$
> 这是给定**有限区间长度**约束下的最大微分熵。

> [!example]- Gauss 分布的微分熵
> 对 $X \sim \mathcal{N}(\mu, \sigma^{2})$，有 $p_{X}(x) = \dfrac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left( - \dfrac{(x - \mu)^{2}}{2 \sigma^{2}} \right)$，因此
> $$
> \begin{align}
> h(X) &= - \dint_{-\infty}^{+\infty} p_{X}(x) \log p_{X}(x) \dif x \\
> &= - \dint_{-\infty}^{+\infty} p_{X}(x) \log \left( \dfrac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left( - \dfrac{(x - \mu)^{2}}{2 \sigma^{2}} \right) \right) \dif x \\
> &= - \dint_{-\infty}^{+\infty} p_{X}(x) \left( - \dfrac{(x - \mu)^{2}}{2 \sigma^{2}} \log \e - \log \sqrt{2 \pi \sigma^{2}} \right) \dif x \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \dint_{-\infty}^{+\infty} (x - \mu)^{2} p_{X}(x) \dif x + \log \sqrt{2 \pi \sigma^{2}} \dint_{-\infty}^{+\infty} p_{X}(x) \dif x \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \mathbb{E}[(X - \mu)^{2}] + \log \sqrt{2 \pi \sigma^{2}} \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \left( \mathbb{E}^{2}\left[ (X - \mu) \right] + \mathrm{Var}\left[ (X - \mu) \right]  \right) + \log \sqrt{2 \pi \sigma^{2}} \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \cdot \sigma^{2} + \log \sqrt{2 \pi \sigma^{2}} = \mark{ \log \sqrt{2 \pi \e \sigma^{2}} }
> \end{align}
> $$
> 这是给定**双边无限区间**约束下的最大微分熵，也是给定**有限方差**约束下的最大微分熵。 

> [!example]- 指数分布的微分熵
> 对 $X \sim \mathrm{Exponential}(\lambda)$，有 $p_{X}(x) = \lambda \e^{-\lambda x}, \quad x \geq 0$，因此
> $$
> \begin{align}
> h(X) &= - \dint_{0}^{+\infty} p_{X}(x) \log p_{X}(x) \dif x \\
> &= - \dint_{0}^{+\infty} \lambda \e^{-\lambda x} \log \left( \lambda \e^{-\lambda x} \right) \dif x \\
> &= - \dint_{0}^{+\infty} \lambda \e^{-\lambda x} \left( \log \lambda - \lambda x \log \e \right) \dif x \\
> &= - \lambda \log \lambda \dint_{0}^{+\infty} \e^{-\lambda x} \dif x + \lambda^{2} \log \e \dint_{0}^{+\infty} x \e^{-\lambda x} \dif x \\
> &= - \lambda \log \lambda \cdot \dfrac{1}{\lambda} + \lambda^{2} \log \e \cdot \dfrac{1}{\lambda^{2}} = \mark{ 1 - \log \lambda }
> \end{align}
> $$
> 这是给定**单边无限区间**约束下的最大微分熵。

#### 联合微分熵、条件微分熵

> [!definition] 联合微分熵
> 考虑两个连续随机变量 $X$ 和 $Y$，其联合概率密度函数为 $p_{X, Y}(x, y)$，则 $X$ 和 $Y$ 的**联合微分熵 (joint differential entropy)** 定义为
> $$
> h(X, Y) = \mathbb{E}_{X, Y} [ - \log p_{X, Y}(X, Y) ] = - \dint_{-\infty}^{+\infty} \dint_{-\infty}^{+\infty} p_{X, Y}(x, y) \log p_{X, Y}(x, y) \dif x \dif y
> $$
> 在没有歧义的情况下，联合微分熵 $h(X,Y)$ 可记为 $h(XY)$。

> [!definition] 条件微分熵
> 考虑 $X$ 和 $Y$ 的联合概率密度函数 $p_{X, Y}(x, y)$，条件随机变量 $X \mid Y$ 的概率密度函数为
> $$
> p_{X \mid Y}(x, y) = \dfrac{p_{X, Y}(x, y)}{p_{Y}(y)}
> $$
> 则以 $Y$ 为条件的 $X$ 的**条件微分熵 (conditional differential entropy)** 定义为
> $$
> h(X \mid Y) = \mathbb{E}_{X, Y} [ - \log p_{X \mid Y}(X, Y) ] = - \dint_{-\infty}^{+\infty} \dint_{-\infty}^{+\infty} p_{X, Y}(x, y) \log p_{X \mid Y}(x, y) \dif x \dif y
> $$

类似地，微分熵也有**链式法则**
$$
h(X, Y) = h(Y) + h(X \mid Y) = h(X) + h(Y \mid X)
$$

#### 微分熵的互信息

> [!definition] 微分熵的互信息
> 连续随机变量 $X$ 和 $Y$ 的**互信息 (mutual information)** 定义为
> $$
> I(X; Y) = h(X) - h(X \mid Y) = h(Y) - h(Y \mid X)
> $$

仍有 $h(X) \ge h(X \mid Y)$，因此 $I(X; Y) \geq 0$。

一般地，
$$
\begin{align}
I(X; Y) &= h(X) - h(X \mid Y) = h(X) + h(Y) - h(X, Y) \\
&= \dint_{-\infty}^{+\infty} \dint_{-\infty}^{+\infty} p_{X, Y}(x, y) \log \dfrac{p_{X, Y}(x, y)}{p_{X}(x) p_{Y}(y)} \dif x \dif y
\end{align}
$$
