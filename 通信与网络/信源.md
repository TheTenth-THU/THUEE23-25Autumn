## 信源的概念

> [!definition] 信源
> 信息的产生者称为**信源 (source of information)**，产生一个**随机过程**发出信息。

### 离散无记忆信源

**离散信源 (discrete source)** 是时间、取值上都离散的信源，可以表示为 
$$
X[k] \in \{ x_{1}, x_{2}, \cdots, x_{N} \}
$$

为简化讨论，可以假设 **$X[k]$ 是独立同分布的随机过程**，即：

> [!definition] 离散无记忆信源
> 持续产生**独立同分布**的符号  $X \in \{ x_{1}, x_{2}, \cdots, x_{N} \}$ 的信源，称为**离散无记忆信源 (discrete memoryless source, DMS)**，记为
> $$
> X \sim \begin{pmatrix}
> x_{1} & x_{2} & \cdots & x_{N} \\
> p_{1} & p_{2} & \cdots & p_{N}
> \end{pmatrix}, \quad
> p_{i} = \mathrm{Pr}\{ X = x_{i} \}
> $$

### 信源编码的基本要求

将信源产生的符号 $X$ 映射为 0, 1 比特串 $f(X)$ 的过程称为**信源编码 (source coding)**，产生的比特串称为**码字 (codeword)**，其长度 $l$ 称为**码长 (code length)**。

+ 若不同 $x_{i}$ 映射出的码长 $l_{i}$ 相同，称为**定长码 (fixed-length code)**；
+ 若不同 $x_{i}$ 映射出的码长 $l_{i}$ 不同，称为**变长码 (variable-length code)**。

#### 定长码的可解码条件

对**定长码**，要求
$$
f(x_{i}) \neq f(x_{j}), \quad \forall i \neq j
$$
因此固定码长 $l$ 应满足
$$
N \leq 2^{l} \implies l \geq \lceil \log N \rceil
$$
此处 $\log$ 均为以 2 为底的对数。

#### 变长码的可解码条件

对**变长码**，要求任意码字不能是另一个码字的前缀，因此又称为**前缀码 (prefix code)**。

引入**平均码长 (average code length)** 
$$
\bar{l} = \sum\limits_{i=1}^{N} p_{i} l_{i}
$$
我们希望在可解码的条件下，使 $\bar{l}$ 尽可能小。

可以证明，信源 $X \sim \begin{pmatrix} x_{1} & x_{2} & \cdots & x_{N} \\ p_{1} & p_{2} & \cdots & p_{N} \end{pmatrix}$ 的最小平均码长为
$$
\bar{l}_{\min} = - \sum\limits_{i=1}^{N} p_{i} \log p_{i}
$$

## 信源的熵

### 离散信源的熵

> [!definition] 离散信源的熵
> 离散无记忆信源 $X \sim \begin{pmatrix} x_{1} & x_{2} & \cdots & x_{N} \\ p_{1} & p_{2} & \cdots & p_{N} \end{pmatrix}$ 的**熵 (entropy)** 定义为
> $$
> H(X) = - \sum\limits_{i=1}^{N} p_{i} \log p_{i}
> $$

$H(X)$ 可以写为
$$
H(X) = \mathbb{E}_{X} [ - \log p(X) ]
$$
因而 $- \log \mathrm{Pr}\{ X = x_{i} \}$ 刻画事件 $\{ X = x_{i} \}$ 所包含的信息量，**概率越小，信息量越大**。

#### 熵的性质

考虑 $H(X) = \bar{l}_{\min}$ 的物理含义，应有 $0 \le H(X) \le \log N$。

+ 当且仅当 $$\exists i \in \{ 1, 2, \cdots,N \}, \quad p_{i} = 1$$ 时，$H(X) = 0$，此时信源没有不确定性，不包含信息；

+ 当且仅当 $$p_{i} = \dfrac{1}{N}, \quad i = 1, 2, \cdots, N$$ 时，$H(X) = \log N$，此时信源不确定性最大，包含信息量最大。
	对离散信源，**均匀分布的不确定度最大**。

#### 联合熵、条件熵

> [!definition] 联合熵
> 考虑两个离散无记忆信源 $X$ 和 $Y$，事件 $\{ X = x_{i}, Y = y_{j} \}$ 的概率为 $p_{ij} = \mathrm{Pr}\{ X = x_{i}, Y = y_{j} \}$，则 $X$ 和 $Y$ 的**联合熵 (joint entropy)** 定义为
> $$
> H(X, Y) = \mathbb{E}_{XY} [ - \log p(X, Y) ]
> = - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log p_{ij}
> $$
> 在没有歧义的情况下，联合熵 $H(X,Y)$ 可记为 $H(XY)$。

联合熵刻画了将 $X$ 和 $Y$ 一起编码所需的最小平均码长，即综合考虑 $X$ 和 $Y$ 的信息量。

> [!definition] 条件熵
> 考虑 $X$ 和 $Y$ 的联合分布 $p_{ij} = \mathrm{Pr}\{ X = x_{i}, Y = y_{j} \}$，在 $Y$ 已知的条件下，事件 $\{ X = x_{i} \}$ 发生的概率为
> $$
> p_{i \mid j} = \mathrm{Pr}\{ X = x_{i} \mid Y = y_{j} \} = \dfrac{\mathrm{Pr}\{ X = x_{i}, Y = y_{j} \}}{\mathrm{Pr}\{ Y = y_{j} \}} = \dfrac{p_{ij}}{p_{j}}
> $$
> 则以 $Y$ 为条件的 $X$ 的**条件熵 (conditional entropy)** 定义为
> $$
> H(X \mid Y) = \mathbb{E}_{XY} [ - \log p(X \mid Y) ] = - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log p_{i \mid j}
> $$

条件熵 $H(X \mid Y)$ 刻画了在已知 $Y$ 的条件下，$X$ 所包含的信息量，即在观测 $Y$ 后 $X$ 残存的不确定度。

> [!note] 熵的通信意义
> 信源的熵 $H(X)$ 刻画了信源 $X$ 所包含的信息量，其值即对 $X$ 进行无失真编码时所需的最小平均码长。
> 
> + 考虑对一个离散无记忆信源 $X$ 编码传输，当**信道速率**（每传输一个信源符号所传输的平均比特数）**$R \geq H(X)$** 时，可以实现**无失真**传输，即信源译码环节可无失真恢复 $X$；
> 
> + 考虑对两个离散无记忆信源 $X$ 和 $Y$ 做联合信源编码传输，当**信道速率 $R \geq H(X, Y)$** 时，在信源译码环节可无失真恢复 $(X, Y)$；
> 
> + 考虑对离散无记忆信源 $X$ 编码传输，编译码器可以共同观测另一个信源 ，当**信道速率 $R \geq H(X \mid Y)$** 时，在信源译码环节可无失真恢复 。

#### 熵的链式法则

> [!theorem] 熵的链式法则
> 对于任意两个离散随机变量 $X$ 和 $Y$，有
> $$
> H(X, Y) = H(Y) + H(X \mid Y) = H(X) + H(Y \mid X)
> $$

进一步地，

+ 若 **$X$ 与 $Y$ 独立**（记为 $X \perp Y$），则 $p_{ij} = p_{i}p_{j}$，$p_{i \mid j} = \dfrac{p_{i}p_{j}}{p_{j}} = p_{i}$，于是
$$
\begin{align}
H(X \mid Y) &= - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log p_{i \mid j} = - \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{i} p_{j} \log p_{i}  \\
&= - \sum\limits_{i=1}^{N} p_{i} \log p_{i} = H(X)
\end{align}
$$
$$
H(XY) = H(X \mid Y) + H(Y) = H(X) + H(Y)
$$
即**独立随机变量的联合熵等于各自熵之和**。观测 $Y$ 不会减少 $X$ 的不确定性。

+ 若 $X$ 是 $Y$ 的**确定性映射**（记为 $X = f(Y)$），则 $p_{i \mid j}$ 要么为 0，要么为 1，因此 $H(X \mid Y) = 0$，于是
$$
H(X, Y) = H(Y)
$$
即**确定性映射不会增加不确定性**。观测 $Y$ 会完全消除 $X$ 的不确定性。

#### 互信息

$H(X \mid Y)$ 表征通过观测 $Y$ 后 $X$ 残存的不确定度，这种观测所消除不确定度的程度，即称为**互信息 (mutual information)**。

显然，有
$$
\begin{align}
H(X) - H(X \mid Y) &= H(X) - \big( H(X, Y) - H(Y) \big) \\
&= H(X) + H(Y) - H(X, Y) \\
&= H(Y) - H(Y \mid X)
\end{align}
$$
即，通过观测 $Y$ 消除的 $X$ 的不确定度，等于通过观测 $X$ 消除的 $Y$ 的不确定度。$X$ 与 $Y$ 的**互信息是对称的**。

> [!definition] 互信息
> 离散随机变量 $X$ 和 $Y$ 的**互信息 (mutual information)** 定义为
> $$
> I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)
> $$

一般地，
$$
I(X; Y) = H(X) + H(Y) - H(X, Y)
= \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{M} p_{ij} \log \dfrac{p_{ij}}{p_{i} p_{j}}
$$

考虑两个离散无记忆信源 $X$ 和 $Y$，

+ 若 $X \perp Y$，则
$$
I(X; Y) = H(X) - H(X \mid Y) = H(X) - H(X) = 0
$$
即**独立信源间没有互信息**；

+ 若 $X = f(Y)$，则
$$
I(X; Y) = H(X) - H(X \mid Y) = H(X) - 0 = H(X)
$$
即**确定性映射的信源间互信息是其全部不确定度**。 

+ $0 \le H(X \mid Y) \le H(X)$，因此 $I(X;Y) \ge 0$，**信源之间不存在欺骗**。

### 连续信源的微分熵

对于连续随机变量 $X$，其概率分布由**概率密度函数 (probability density function, PDF)** $p_{X}(x)$ 描述，满足
$$
\dint_{-\infty}^{+\infty} p_{X}(x) \dif x = 1, \quad p_{X}(x) \geq 0
$$

> [!definition] 微分熵
> **微分熵 (differential entropy)** 定义为
> $$
> h(X) = \mathbb{E}_{X} [ - \log p_{X}(X) ] = - \dint_{-\infty}^{+\infty} p_{X}(x) \log p_{X}(x)  \dif x
> $$

微分熵 $h(X)$ **刻画了信源 $X$ 的相对不确定度**，其值可以为负数。

> [!example]- Gauss 分布的微分熵
> 对 $X \sim \mathcal{N}(\mu, \sigma^{2})$，有 $p_{X}(x) = \dfrac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left( - \dfrac{(x - \mu)^{2}}{2 \sigma^{2}} \right)$，因此
> $$
> \begin{align}
> h(X) &= - \dint_{-\infty}^{+\infty} p_{X}(x) \log p_{X}(x) \dif x \\
> &= - \dint_{-\infty}^{+\infty} p_{X}(x) \log \left( \dfrac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left( - \dfrac{(x - \mu)^{2}}{2 \sigma^{2}} \right) \right) \dif x \\
> &= - \dint_{-\infty}^{+\infty} p_{X}(x) \left( - \dfrac{(x - \mu)^{2}}{2 \sigma^{2}} \log \e - \log \sqrt{2 \pi \sigma^{2}} \right) \dif x \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \dint_{-\infty}^{+\infty} (x - \mu)^{2} p_{X}(x) \dif x + \log \sqrt{2 \pi \sigma^{2}} \dint_{-\infty}^{+\infty} p_{X}(x) \dif x \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \mathbb{E}[(X - \mu)^{2}] + \log \sqrt{2 \pi \sigma^{2}} \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \left( \mathbb{E}^{2}\left[ (X - \mu) \right] + \mathrm{Var}\left[ (X - \mu) \right]  \right) + \log \sqrt{2 \pi \sigma^{2}} \\
> &= \dfrac{\log \e}{2 \sigma^{2}} \cdot \sigma^{2} + \log \sqrt{2 \pi \sigma^{2}} = \mark{ \log \sqrt{2 \pi \e \sigma^{2}} }
> \end{align}
> $$

#### 联合微分熵、条件微分熵

> [!definition] 联合微分熵
> 考虑两个连续随机变量 $X$ 和 $Y$，其联合概率密度函数为 $p_{X, Y}(x, y)$，则 $X$ 和 $Y$ 的**联合微分熵 (joint differential entropy)** 定义为
> $$
> h(X, Y) = \mathbb{E}_{X, Y} [ - \log p_{X, Y}(X, Y) ] = - \dint_{-\infty}^{+\infty} \dint_{-\infty}^{+\infty} p_{X, Y}(x, y) \log p_{X, Y}(x, y) \dif x \dif y
> $$
> 在没有歧义的情况下，联合微分熵 $h(X,Y)$ 可记为 $h(XY)$。

> [!definition] 条件微分熵
> 考虑 $X$ 和 $Y$ 的联合概率密度函数 $p_{X, Y}(x, y)$，条件随机变量 $X \mid Y$ 的概率密度函数为
> $$
> p_{X \mid Y}(x, y) = \dfrac{p_{X, Y}(x, y)}{p_{Y}(y)}
> $$
> 则以 $Y$ 为条件的 $X$ 的**条件微分熵 (conditional differential entropy)** 定义为
> $$
> h(X \mid Y) = \mathbb{E}_{X, Y} [ - \log p_{X \mid Y}(X, Y) ] = - \dint_{-\infty}^{+\infty} \dint_{-\infty}^{+\infty} p_{X, Y}(x, y) \log p_{X \mid Y}(x, y) \dif x \dif y
> $$

类似地，微分熵也有**链式法则**
$$
h(X, Y) = h(Y) + h(X \mid Y) = h(X) + h(Y \mid X)
$$

#### 微分熵的互信息

> [!definition] 微分熵的互信息
> 连续随机变量 $X$ 和 $Y$ 的**互信息 (mutual information)** 定义为
> $$
> I(X; Y) = h(X) - h(X \mid Y) = h(Y) - h(Y \mid X)
> $$

一般地，
$$
\begin{align}
I(X; Y) &= h(X) - h(X \mid Y) = h(X) + h(Y) - h(X, Y) \\
&= \dint_{-\infty}^{+\infty} \dint_{-\infty}^{+\infty} p_{X, Y}(x, y) \log \dfrac{p_{X, Y}(x, y)}{p_{X}(x) p_{Y}(y)} \dif x \dif y
\end{align}
$$

## 模拟信源的数字编码

模拟信源是**时间连续、幅值连续**的信源，其输出 $s(t)$ 是一个连续时间的[[随机过程的时域分析#^b4e44e|随机过程]]，将其数字化的过程如下：

![[模拟信源的数字编码.png|模拟信源的数字编码]]

在信道另一侧，接收端对接收到的二值序列进行**译码 (digital decoding)**，恢复出数字信号，然后通过**电平重建 (level reconstruction)** 和**内插 (interpolation)**，即恢复出模拟信号 $\hat{s}(t)$。

### 抽样与内插恢复

**抽样 (sampling)** 是将连续时间信号 $s(t)$ 在时间上离散化的过程，得到离散时间信号 $x[k] = s(kT_{\mathrm{s}})$，其中 $T_{\mathrm{s}}$ 是**抽样周期 (sampling period)**，$f_{\mathrm{s}} = \dfrac{1}{T_{\mathrm{s}}}$ 是**抽样频率 (sampling frequency)**。

> [!theorem] Nyquist 抽样定理
> 设 $s(t)$ 是**带宽受限**的信号，其频谱 $S(f) = \mathcal{F}\left\{ s(t) \right\}$ 满足
> $$
> S(f) = 0, \quad |f| > W
> $$
> 则当抽样频率 $f_{\mathrm{s}} \ge 2W$ 时，可以从抽样所得信号 $x[k] = s(kT_{\mathrm{s}})$ **无失真**地恢复出 $s(t)$。这个最低抽样频率 $f_{\mathrm{s}} = 2W$ 称为 **Nyquist 频率 (Nyquist frequency)**。

从序列 $x[k]$ 恢复出 $s(t)$，可以先以 $x[k]$ 调制冲激序列得到 $\t{s}(t) = \sum\limits_{k} x[k] \delta(t-kT_{\mathrm{s}})$，其 Fourier 变换为
$$
\t{S}(f) = \mathcal{F}\left\{ \t{s}(t) \right\} 
= \sum\limits_{k} x[k] \mathcal{F} \left\{ \delta(t-kT_{\mathrm{s}}) \right\}
= \sum\limits_{k} s(kT_{\mathrm{s}}) \e^{-\J 2 \pi f k T_{\mathrm{s}}}
$$
另一方面，
$$
\begin{align}
\t{S}(f) &= \mathcal{F}\left\{ s(t) \sum\limits_{k} \delta(t-kT_{\mathrm{s}}) \right\} 
= \dfrac{1}{2\pi}S(f) * \mathcal{F}\left\{ \sum\limits_{k} \delta(t-kT_{\mathrm{s}}) \right\} \\
&= \dfrac{1}{T_{\mathrm{s}}} S(f) * \sum\limits_{k} \delta \left( f - k f_{\mathrm{s}} \right) \\
\end{align}
$$
因而需使用增益为 $T_{\mathrm{s}}$ 的**低通滤波器 (low-pass filter, LPF)** 截去 $\t{S}(f)$ 中 $|f| > W$ 的部分，才能恢复出 $S(f)$，输出即为
$$
\hat{S}(f) = \dfrac{1}{f_{\mathrm{s}}} \sum\limits_{k} s(kT_{\mathrm{s}}) \e^{-\J 2 \pi f k T_{\mathrm{s}}} \times \mathbb{1}_{|f| \leq W}
$$
其中 $\mathbb{1}_{P}$ 为事件 $P$ 的指示函数。由此，时域恢复的信源为
$$
\begin{align}
\hat{s}(t) &= \mathcal{F}^{-1}\left\{ \hat{S}(f) \right\} = \dint_{-\infty}^{+\infty} \hat{S}(f) \e^{\J 2 \pi f t} \dif f 
= \dint_{-W}^{W} \dfrac{1}{f_{\mathrm{s}}} \sum\limits_{k} s(kT_{\mathrm{s}}) \e^{-\J 2 \pi f k T_{\mathrm{s}}} \e^{\J 2 \pi f t} \dif f \\
&= \dfrac{1}{f_{\mathrm{s}}} \sum\limits_{k} s(kT_{\mathrm{s}}) \dint_{-W}^{W} \e^{\J 2 \pi f (t - kT_{\mathrm{s}})} \dif f 
= \sum\limits_{k} s(kT_{\mathrm{s}}) \dfrac{\sin (2\pi W(t-kT_{\mathrm{s}}))}{\pi f_{\mathrm{s}} (t-kT_{\mathrm{s}})}
\end{align}
$$
令抽样频率 $f_{\mathrm{s}} = 2W$ 取 Nyquist 频率，则
$$
\hat{s}(t) = \sum\limits_{k} s\left( \dfrac{k}{2W} \right) \sinc \left( 2Wt - k \right)
$$

> [!note] 抽样前的预滤波
> 实际采集到的真实信号一定是全频带的信号，为避免混叠 (aliasing) 干扰，通常先通过**预滤波器 (pre-filter)** 截去高频成分，保留期望频带内的信号，然后再进行抽样。
> 
> 人类的语音信号主要能量集中在 300  $\sim$ 3400 Hz，通常预滤波器的带宽为 3.4 kHz，抽样频率为 8 kHz。

### 量化与电平重建

**量化 (quantization)** 是将幅值连续的信号 $x[k]$ 离散化为数字信号 $\hat{x}[k]$ 的过程。设量化器 $Q$ 有 $L$ 个量化级别，则第 $i$ 个量化区间上
$$
Q(x) = y_{i}, \quad x \in (t_{i}, t_{i + 1}], \quad i = 1, 2, \cdots, L
$$
其中，$y_{i}$ 是**表示电平**，$t_{i}$ 是**分层电平**，$I_{i} = (t_{i}, t_{i+1}]$ 称为第 $i$ 个**量化区间 (quantization interval)**，其长度 $\varDelta_{i} = t_{i+1} - t_{i}$ 称为第 $i$ 个**量化间隔 (quantization step)**。

当对每个 $i = 1, 2, \cdots, L$ 有 $\varDelta_{i} \equiv \varDelta$ 时，称为**均匀量化 (uniform quantization)**，否则称为**非均匀量化 (non-uniform quantization)**。

#### 量化误差

量化器 $Q$ 对输入 $x$ 的**量化误差 (quantization error)** 定义为
$$
e(x) = x - Q(x)
$$
因为 $x$ 可看作**随机变量 $X$**，$e(x)$ 也是随机变量。良好的量化器应使得 $\mathbb{E}\left[ e(X) \right] = 0$，此时其方差为
$$
\begin{align}
\sigma^{2} = \mathbb{E}\left[ e^{2}(X) \right] &= \dint_{-\infty}^{+\infty} (x - Q(x))^{2} p_{X}(x) \dif x \\
&= \sum\limits_{i=1}^{L} \dint_{t_{i}}^{t_{i+1}} (x - y_{i})^{2} p_{X}(x) \dif x
\end{align}
$$

> [!definition] 量化的均方误差
> 对量化器 $Q$，其量化误差的方差 
> $$
> \sigma^{2} = \mathbb{E}\left[ e^{2}(X) \right] = \sum\limits_{i=1}^{L} \dint_{t_{i}}^{t_{i+1}} (x - y_{i})^{2} p_{X}(x) \dif x
> $$
> 称为**量化的均方误差 (mean square error, MSE)**。

一般而言，功率较大的信源对量化噪声的容忍程度也比较高，因而我们可以引入

> [!definition] 量化器的信噪比
> 量化器 $Q$ 的**信噪比 (signal-to-noise ratio, SNR)** 定义为
> $$
> \mathrm{SNR}_{\mathrm{q}} = \dfrac{\mathbb{E}\left[ X^{2} \right]}{\mathbb{E}\left[ e^{2}(X) \right]} = \dfrac{\dint_{-\infty}^{\infty} x^{2} p_{X}(x) \dif x}{\sum\limits_{i=1}^{L} \dint_{t_{i}}^{t_{i+1}} (x - y_{i})^{2} p_{X}(x) \dif x}
> $$

